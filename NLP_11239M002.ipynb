{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMUhGuRxicBT2wgJyZZSx+t",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Thribhuvan03/11239M002_daalab/blob/main/NLP_11239M002.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**LEMMATIZATION**"
      ],
      "metadata": {
        "id": "nosrpdwkqlTv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet', quiet=True)\n",
        "nltk.download('omw-1.4', quiet=True)\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "words = [\"cats\", \"running\", \"better\", \"studies\"]\n",
        "for w in words:\n",
        "    print(w, \"→\", lemmatizer.lemmatize(w))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t0eRoJO1qk1z",
        "outputId": "f6499daa-a0cd-446a-97a3-8048d0f05c5e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cats → cat\n",
            "running → running\n",
            "better → better\n",
            "studies → study\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**NORMALIZATION**"
      ],
      "metadata": {
        "id": "yjINXx68qjhO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "text = \"This is an Example: Normalizing Text in NLP!!!\"\n",
        "text = text.lower()                     # lowercase\n",
        "text = re.sub(r'[^\\w\\s]', '', text)     # remove punctuation\n",
        "print(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xvxs8lEXq19w",
        "outputId": "09e9bb1b-bdaf-493e-93ec-80a9482398df"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "this is an example normalizing text in nlp\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TOKENIZATION**"
      ],
      "metadata": {
        "id": "-8VYaGMirB_D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab', quiet=True)\n",
        "from nltk.tokenize import word_tokenize\n",
        "text = \"I love learning NLP with ChatGPT!\"\n",
        "print(word_tokenize(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_AZUtLrqrPnj",
        "outputId": "66de4cfa-b359-4d6f-d474-f0fef7298249"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I', 'love', 'learning', 'NLP', 'with', 'ChatGPT', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**STEMMING**"
      ],
      "metadata": {
        "id": "6MYf2s74rTm0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "stemmer = PorterStemmer()\n",
        "words = [\"running\", \"flies\", \"easily\", \"fairly\"]\n",
        "for w in words:\n",
        "    print(w, \"→\", stemmer.stem(w))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "trT2VvzZrTJ5",
        "outputId": "73119b66-657f-4b8c-ff0e-9f0bd54ee6e6"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "running → run\n",
            "flies → fli\n",
            "easily → easili\n",
            "fairly → fairli\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**MORPHOLOGY**"
      ],
      "metadata": {
        "id": "ytEEac1oraz9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy, warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "text = \"The cats are running quickly\"\n",
        "doc = nlp(text)\n",
        "for token in doc:\n",
        "    print(token.text, \"→\", token.lemma_, \"|\", token.pos_, \"|\", token.morph)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jF7sib1Lrhy-",
        "outputId": "03f04a52-0ce7-4493-ebf7-ae0c1fdc34ac"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The → the | DET | Definite=Def|PronType=Art\n",
            "cats → cat | NOUN | Number=Plur\n",
            "are → be | AUX | Mood=Ind|Tense=Pres|VerbForm=Fin\n",
            "running → run | VERB | Aspect=Prog|Tense=Pres|VerbForm=Part\n",
            "quickly → quickly | ADV | \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SPELLING CORRECTION**"
      ],
      "metadata": {
        "id": "e0Xs1nZ-rjMX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install textblob -q\n",
        "from textblob import TextBlob\n",
        "text = \"I lik to lern naturall langauge procesing\"\n",
        "blob = TextBlob(text)\n",
        "print(blob.correct())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HWpaJ7jUrbfu",
        "outputId": "4fb5233b-3b92-4d75-c14d-c73d9168365d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I like to learn natural language processing\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**DEDUCTION**"
      ],
      "metadata": {
        "id": "DcY3p0Qmrzmv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.sem import Expression\n",
        "from nltk.inference import ResolutionProver\n",
        "\n",
        "read_expr = Expression.fromstring\n",
        "\n",
        "kb = [\n",
        "    read_expr('man(Socrates)'),\n",
        "    read_expr('all x (man(x) -> mortal(x))')\n",
        "]\n",
        "goal = read_expr('mortal(Socrates)')\n",
        "print(ResolutionProver().prove(goal, kb))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lCZQh3Uur0Da",
        "outputId": "cd1dc459-461e-4078-ca19-7f6165ebbd4a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**UNIGRAM**"
      ],
      "metadata": {
        "id": "K9QAfkJpr9IH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"I love natural language processing and I love coding\"\n",
        "\n",
        "words = text.split()\n",
        "\n",
        "freq = {}\n",
        "for word in words:\n",
        "    freq[word] = freq.get(word, 0) + 1\n",
        "\n",
        "print(freq)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XND-0mmWr9ju",
        "outputId": "1e411da0-6174-456b-f817-4cd74f508c22"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'I': 2, 'love': 2, 'natural': 1, 'language': 1, 'processing': 1, 'and': 1, 'coding': 1}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**BIGRAM**"
      ],
      "metadata": {
        "id": "7GFBpiG_r-iP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"I love natural language processing and I love coding\"\n",
        "\n",
        "words = text.split()\n",
        "\n",
        "bigrams = []\n",
        "for i in range(len(words) - 1):\n",
        "    bigrams.append((words[i], words[i+1]))\n",
        "\n",
        "freq = {}\n",
        "for bigram in bigrams:\n",
        "    freq[bigram] = freq.get(bigram, 0) + 1\n",
        "\n",
        "print(freq)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XuNAOMHEr-EB",
        "outputId": "1f397311-1f67-4fa7-e2ce-0085744d2e89"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{('I', 'love'): 2, ('love', 'natural'): 1, ('natural', 'language'): 1, ('language', 'processing'): 1, ('processing', 'and'): 1, ('and', 'I'): 1, ('love', 'coding'): 1}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TRIGRAM**"
      ],
      "metadata": {
        "id": "FCoGc3zYsQ3m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"I love natural language processing and I love coding\"\n",
        "\n",
        "words = text.split()\n",
        "\n",
        "trigrams = []\n",
        "for i in range(len(words) - 2):\n",
        "    trigrams.append((words[i], words[i+1], words[i+2]))\n",
        "\n",
        "freq = {}\n",
        "for trigram in trigrams:\n",
        "    freq[trigram] = freq.get(trigram, 0) + 1\n",
        "\n",
        "print(freq)\n"
      ],
      "metadata": {
        "id": "hqTSNXcRsULa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**N-GRAM SMOOTHINBG**"
      ],
      "metadata": {
        "id": "cCmktVLpsTvR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "text = \"I love NLP I love machine learning\"\n",
        "words = text.split()\n",
        "V = len(set(words))  # Vocabulary size\n",
        "unigrams = Counter(words)\n",
        "bigrams = Counter([(words[i], words[i+1]) for i in range(len(words)-1)])\n",
        "def laplace_prob(w1, w2):\n",
        "    return (bigrams[(w1, w2)] + 1) / (unigrams[w1] + V)\n",
        "print(\"P(love | I) =\", laplace_prob(\"I\", \"love\"))\n",
        "print(\"P(NLP | love) =\", laplace_prob(\"love\", \"NLP\"))\n",
        "print(\"P(machine | NLP) =\", laplace_prob(\"NLP\", \"machine\"))\n",
        "print(\"P(learning | machine) =\", laplace_prob(\"machine\", \"learning\"))\n",
        "print(\"P(unknown | NLP) =\", laplace_prob(\"NLP\", \"unknown\"))  # unseen word"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_qsdB-nkseEd",
        "outputId": "6475d899-fb0d-49af-d1ff-be1cc98f6140"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "P(love | I) = 0.42857142857142855\n",
            "P(NLP | love) = 0.2857142857142857\n",
            "P(machine | NLP) = 0.16666666666666666\n",
            "P(learning | machine) = 0.3333333333333333\n",
            "P(unknown | NLP) = 0.16666666666666666\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**POS TAGGING**"
      ],
      "metadata": {
        "id": "fdnwVazgsw7L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger_eng', quiet=True)\n",
        "text = \"I love learning NLP\"\n",
        "words = nltk.word_tokenize(text)\n",
        "print(nltk.pos_tag(words))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "myRofFvBs4Gu",
        "outputId": "5f42a7d7-dbb5-4b37-d106-e2d97090a895"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('I', 'PRP'), ('love', 'VBP'), ('learning', 'VBG'), ('NLP', 'NNP')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**HMM TAGGING**"
      ],
      "metadata": {
        "id": "ZTSVnTvvs97z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tag import hmm\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "train_data = [[\n",
        "    ('I', 'PRON'),\n",
        "    ('love', 'VERB'),\n",
        "    ('dogs', 'NOUN')\n",
        "], [\n",
        "    ('You', 'PRON'),\n",
        "    ('love', 'VERB'),\n",
        "    ('cats', 'NOUN')\n",
        "]]\n",
        "trainer = hmm.HiddenMarkovModelTrainer()\n",
        "tagger = trainer.train_supervised(train_data)\n",
        "sentence = ['I', 'love', 'cats']\n",
        "print(tagger.tag(sentence))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WvConiTgtNuE",
        "outputId": "6773054b-e1f5-4200-8878-089ca1da79c8"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('I', 'PRON'), ('love', 'VERB'), ('cats', 'NOUN')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**BRILL POS TAGGER**"
      ],
      "metadata": {
        "id": "X1ZRwP7ntRxj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tag import brill, brill_trainer, UnigramTagger\n",
        "\n",
        "nltk.download('treebank', quiet=True)\n",
        "nltk.download('universal_tagset', quiet=True)\n",
        "data = nltk.corpus.treebank.tagged_sents(tagset='universal')[:3000]\n",
        "uni = UnigramTagger(data)\n",
        "tagger = brill_trainer.BrillTaggerTrainer(uni, brill.fntbl37()).train(data)\n",
        "print(tagger.tag(\"I love learning NLP\".split()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZSdp9lcmtYDx",
        "outputId": "373fed68-4399-48ca-8be0-84f12261e9be"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('I', 'PRON'), ('love', None), ('learning', 'NOUN'), ('NLP', None)]\n"
          ]
        }
      ]
    }
  ]
}